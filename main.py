# -*- coding = utf-8 -*-
# @Time : 2025/11/13 22:44
# @Author : åˆ˜èµå
# @File : workflow.py
# @Software : PyCharm
import json
import requests
import time
import os
import argparse  # ==================== æ–°å¢å¯¼å…¥ ====================
from typing import Dict, Any, List, Tuple
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3
from collections import OrderedDict
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import Manager
import math

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==================== ç¦ç”¨ç³»ç»Ÿä»£ç† ====================
os.environ['NO_PROXY'] = '*'
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['http_proxy'] = ''
os.environ['https_proxy'] = ''

# ==================== é…ç½®å‚æ•° ====================
ACCESS_TOKEN = "xxxxxxxxxx"
WORKFLOW_ID = "xxxxxxxxxx"
INPUT_FILE = "test_file_path"
OUTPUT_FILE = "prediction.jsonl"

# å¤šè¿›ç¨‹é…ç½®
NUM_PROCESSES = 4  # ğŸ”§ å¯ä¿®æ”¹è¿›ç¨‹æ•°

# APIé…ç½®
API_URL = "https://api.coze.cn/v1/workflow/run"
MAX_RETRIES = 3
TIMEOUT = 240
RETRY_DELAY = 5
VERIFY_SSL = False


# ==================== åˆ›å»ºSession ====================
def create_session():
    """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„Sessionï¼Œç¦ç”¨ä»£ç†"""
    session = requests.Session()
    session.proxies = {'http': None, 'https': None, 'no_proxy': '*'}
    session.trust_env = False

    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["POST"]
    )

    adapter = HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=10,
        pool_maxsize=10
    )

    session.mount("http://", adapter)
    session.mount("https://", adapter)

    return session


# ==================== å¤„ç†è¾“å‡ºæ ¼å¼ ====================
def process_output(data_str: str) -> Dict[str, Any]:
    """å¤„ç†APIè¿”å›çš„dataå­—æ®µï¼Œæå–å¹¶æ ¼å¼åŒ–è¾“å‡º"""
    try:
        if isinstance(data_str, str):
            data_obj = json.loads(data_str)
        else:
            data_obj = data_str

        if isinstance(data_obj, dict) and 'output' in data_obj:
            output_str = data_obj['output']
            if isinstance(output_str, str):
                output_obj = json.loads(output_str)
            else:
                output_obj = output_str
        else:
            output_obj = data_obj

        result = OrderedDict()
        if 'id' in output_obj:
            result['id'] = output_obj['id']
        if 'answer1' in output_obj:
            result['answer1'] = output_obj['answer1']
        if 'answer2' in output_obj:
            result['answer2'] = output_obj['answer2']

        for key in sorted(output_obj.keys()):
            if key not in result:
                result[key] = output_obj[key]

        return dict(result)

    except (json.JSONDecodeError, TypeError) as e:
        return {"raw_output": data_str}


# ==================== è°ƒç”¨å·¥ä½œæµ ====================
def call_workflow(session: requests.Session, id_value: Any, fact: str) -> Dict[str, Any]:
    """è°ƒç”¨å·¥ä½œæµAPIï¼Œå¸¦è¶…æ—¶é‡è¯•æœºåˆ¶"""
    headers = {
        "Authorization": f"Bearer {ACCESS_TOKEN}",
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    payload = {
        "workflow_id": WORKFLOW_ID,
        "parameters": {
            "id": id_value,
            "fact": fact
        }
    }

    last_exception = None

    for attempt in range(MAX_RETRIES):
        try:
            response = session.post(
                API_URL,
                headers=headers,
                json=payload,
                timeout=TIMEOUT,
                verify=VERIFY_SSL,
                proxies={'http': None, 'https': None}
            )

            response.raise_for_status()
            result = response.json()

            if result.get("code") == 0:
                data_str = result.get("data", "")
                return process_output(data_str)
            else:
                error_msg = result.get("msg", "Unknown error")
                error_code = result.get("code")

                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY)
                    continue
                else:
                    raise Exception(f"APIé”™è¯¯ (code: {error_code}): {error_msg}")

        except requests.exceptions.Timeout:
            last_exception = TimeoutError("è¯·æ±‚è¶…æ—¶")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        except requests.exceptions.RequestException as e:
            last_exception = e
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        except Exception as e:
            last_exception = e
            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

    if last_exception:
        raise Exception(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {str(last_exception)}")
    else:
        raise Exception("æœªçŸ¥é”™è¯¯ï¼Œå¤„ç†å¤±è´¥")


# ==================== å¤„ç†å•ä¸ªä»»åŠ¡ ====================
def process_single_task(task_info: Tuple[int, int, Any, str]) -> Tuple[bool, int, int, Any, Dict, str]:
    """
    å¤„ç†å•ä¸ªä»»åŠ¡

    Args:
        task_info: (index, line_num, id_value, fact)

    Returns:
        (success, index, line_num, id_value, result_dict, error_msg)
    """
    index, line_num, id_value, fact = task_info
    session = create_session()

    try:
        result = call_workflow(session, id_value, fact)
        return (True, index, line_num, id_value, result, "")
    except Exception as e:
        error_msg = str(e)
        return (False, index, line_num, id_value, {}, error_msg)
    finally:
        session.close()
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«


# ==================== æ‰¹å¤„ç†å‡½æ•° ====================
def process_batch(batch_tasks: List[Tuple], progress_dict: Dict, lock, process_id: int) -> List[Tuple]:
    """
    å¤„ç†ä¸€æ‰¹ä»»åŠ¡

    Args:
        batch_tasks: ä»»åŠ¡åˆ—è¡¨
        progress_dict: å…±äº«è¿›åº¦å­—å…¸
        lock: è¿›ç¨‹é”
        process_id: è¿›ç¨‹ID

    Returns:
        ç»“æœåˆ—è¡¨
    """
    results = []

    for idx, task in enumerate(batch_tasks, 1):
        result = process_single_task(task)
        results.append(result)

        # æ›´æ–°è¿›åº¦
        with lock:
            progress_dict[process_id] = idx
            total = sum(progress_dict.values())
            print(f"\rè¿›ç¨‹{process_id} [{idx}/{len(batch_tasks)}] | æ€»è¿›åº¦: {total}",
                  end="", flush=True)

    return results


# ==================== è¯»å–è¾“å…¥æ–‡ä»¶ ====================
def load_input_data(filepath: str) -> List[Tuple[int, int, Any, str]]:
    """
    è¯»å–è¾“å…¥æ–‡ä»¶

    Returns:
        [(index, line_num, id_value, fact), ...]
    """
    tasks = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue

            try:
                data = json.loads(line)
                id_value = data.get("id")
                fact = data.get("fact")

                if id_value is not None and fact is not None:
                    tasks.append((len(tasks), line_num, id_value, fact))

            except json.JSONDecodeError:
                continue

    return tasks


# ==================== åˆ†å‰²ä»»åŠ¡ ====================
def split_tasks(tasks: List, num_chunks: int) -> List[List]:
    """å°†ä»»åŠ¡åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡"""
    chunk_size = math.ceil(len(tasks) / num_chunks)
    return [tasks[i:i + chunk_size] for i in range(0, len(tasks), chunk_size)]


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    # ==================== æ–°å¢ï¼šè§£æå‘½ä»¤è¡Œå‚æ•° ====================
    parser = argparse.ArgumentParser(description='Workflow Process')
    parser.add_argument('--pred_file', type=str, default=INPUT_FILE, help='Input file path')
    args = parser.parse_args()
    current_input_file = args.pred_file
    # ==========================================================

    print("=" * 70)
    print("å·¥ä½œæµAPIè°ƒç”¨ç¨‹åº - å¤šè¿›ç¨‹ç‰ˆæœ¬")
    print("=" * 70)
    print(f"è¾“å…¥æ–‡ä»¶: {current_input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"è¿›ç¨‹æ•°é‡: {NUM_PROCESSES}")
    print(f"å·¥ä½œæµID: {WORKFLOW_ID}")
    print(f"è¶…æ—¶è®¾ç½®: {TIMEOUT}ç§’, æœ€å¤§é‡è¯•: {MAX_RETRIES}æ¬¡")
    print("=" * 70)
    print()

    # è¯»å–è¾“å…¥æ•°æ®
    print("ğŸ“– æ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶...")
    try:
        tasks = load_input_data(current_input_file)
        print(f"  âœ“ å…±åŠ è½½ {len(tasks)} ä¸ªä»»åŠ¡")
    except FileNotFoundError:
        print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{current_input_file}'")
        return
    except Exception as e:
        print(f"âœ— æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        return

    if not tasks:
        print("âœ— æ²¡æœ‰æœ‰æ•ˆçš„ä»»åŠ¡æ•°æ®")
        return

    # åˆ†å‰²ä»»åŠ¡
    batches = split_tasks(tasks, NUM_PROCESSES)
    print(f"\nğŸ“¦ ä»»åŠ¡åˆ†é…:")
    for i, batch in enumerate(batches, 1):
        print(f"  è¿›ç¨‹{i}: {len(batch)} ä¸ªä»»åŠ¡")
    print()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(OUTPUT_FILE)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # å¤šè¿›ç¨‹å¤„ç†
    print("ğŸš€ å¼€å§‹å¤šè¿›ç¨‹å¤„ç†...")
    print("=" * 70)

    start_time = time.time()
    all_results = []

    # ä½¿ç”¨Manageråˆ›å»ºå…±äº«å¯¹è±¡
    manager = Manager()
    progress_dict = manager.dict({i: 0 for i in range(1, len(batches) + 1)})
    lock = manager.Lock()

    try:
        with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            futures = {
                executor.submit(process_batch, batch, progress_dict, lock, idx): idx
                for idx, batch in enumerate(batches, 1)
            }

            for future in as_completed(futures):
                process_id = futures[future]
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                except Exception as e:
                    print(f"\nâœ— è¿›ç¨‹{process_id}å‘ç”Ÿé”™è¯¯: {str(e)}")

        print("\n" + "=" * 70)

    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        return
    except Exception as e:
        print(f"\nâœ— å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return

    # å†™å…¥ç»“æœ
    print("\nğŸ’¾ æ­£åœ¨å†™å…¥ç»“æœ...")

    # æŒ‰indexæ’åº
    all_results.sort(key=lambda x: x[1])

    success_count = 0
    failed_items = []

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
        for success, index, line_num, id_value, result_dict, error_msg in all_results:
            if success:
                output_line = json.dumps(result_dict, ensure_ascii=False, separators=(',', ':'))
                outfile.write(output_line + '\n')
                success_count += 1
            else:
                failed_items.append({
                    'line_num': line_num,
                    'id': id_value,
                    'error': error_msg
                })

    # ç»Ÿè®¡ä¿¡æ¯
    elapsed_time = time.time() - start_time
    error_count = len(failed_items)

    print("\n" + "=" * 70)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("=" * 70)
    print(f"âœ“ æˆåŠŸ: {success_count} æ¡")
    print(f"âœ— å¤±è´¥: {error_count} æ¡")
    print(f"â± æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    if len(tasks) > 0:
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {len(tasks) / elapsed_time:.2f} æ¡/ç§’")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print("=" * 70)

    # æ‰“å°å¤±è´¥çš„è¡Œ
    if failed_items:
        print("\n" + "=" * 70)
        print(f"âŒ å¤±è´¥çš„ä»»åŠ¡è¯¦æƒ… (å…± {len(failed_items)} æ¡):")
        print("=" * 70)
        for item in failed_items:
            print(f"\nè¡Œå·: {item['line_num']}")
            print(f"ID: {item['id']}")
            print(f"é”™è¯¯: {item['error']}")
            print("-" * 70)
        print("=" * 70)
    else:
        print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å‡å¤„ç†æˆåŠŸï¼")


if __name__ == "__main__":
    main()